\documentclass[a4paper,twoside,11pt]{article}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{fancyhdr}
\usepackage{newprog1e}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{graphics}
\usepackage{cmap}
\graphicspath{{images/}}
\tolerance=1000

\newcommand{\defi}{\stackrel{\mathrm{def}}{=}}

\numberwithin{equation}{section}
\newtheorem{theorem}{Теорема}

\newtheorem{definition}{Определение}
\newtheorem{theorem_ru}{Теорема}
\newtheorem{statement}{Утверждение}

\journalnumber{4}
\curyear{2018}
\authorlist{V.A. Frolov$^{1,2}$, V. V. Sanzharov$^{3}$}
\titlehead{Modern problems of integration in computer graphics applications and ways to solve them}
\headerdef

\udk{004.921}
\rubrika{COMPUTER GRAPHICS}
\dateinput{08.02.2018}

\rusabstr{ Integration layer between digital content creation software (DCCS) and rendering software in a form of specialized database is proposed in this paper. In our approach, we focus on providing fast 3D-scene updates, ability to work with large digital assets (not fitting into memory), importing and exporting arbitrary parameters, serialization, convenient debugging tools and distributed rendering. Such database can be used as means to integrate different rendering engines with DCCS and also to transfer data between different DCCS. }

\author{
{\bfseries V.A.\,Frolov$^1,^2$,  V.V.\,Sanzharov$^3$}
\\ {\itshape $^1$ Keldysh Institute of Applied Mathematics (Russian Academy of Sciences), Moscow, Russia.}
\\ {\itshape $^2$ Moscow State University, Moscow, Russia. }
\\ {\itshape $^3$ Gubkin russian state university of oil and gas, Moscow, Russia. }
\\ {\itshape E-mail: vfrolov@graphics.cs.msu.ru, vs@asugubkin.ru, vlgal@gin.keldysh.ru }}
\title{Modern problems of software integration in computer graphics applications and ways to solve them}
%\thanks{~}
%\date{}

\begin{document}

\maketitle
\setcounter{page}{3}

\section{Introduction} \label{intro}

Fast growth of computing power in the past 20 years gave rise to new research and industrial fields. Data volumes have grown from megabytes to gigabytes and performance has raised from megaflops to teraflops. But it's not only about the numbers, more important is how it influences interaction with the user. As technology matures, it generally becomes more accessible and friendly for the users. This also holds true for digital content creation. Since rendering is inevitably tied with such applications, rendering engines' developers need to answer the challenges created by the need to improve user experience. In our work we aim to answer these challenges with a database oriented approach to rendering engine infrastructure design. 

\paragraph{Modern digital content creation process requirements}

\begin{enumerate}
	
\item \textit{Interactivity} of digital content creation process (WYSIWYG paradigm). Working in the interactive and non-interactive modes can be thought of as editing a text document in MS Word and in \LaTeX. The latter has certain advantages, however, creating a document in MS Word is faster.

\item \textit{No restrictions on memory}. The whole 3D scene may not fit into RAM of a single computer, but content creation process and rendering (at the very least in preview mode) should not have delays \cite{outofcore}. 

\item \textit{Parameters variability} and extensibility of both digital content creation software (DCCS) and rendering systems. Different projects may require different computer graphics algorithms to be used (like for hair and fur). It's quite common for post-production studios to create their own rendering plugins both for DCCS and renderers \cite{mrhair}. 

\item \textit{Serialization}, import and export. Artists reuse variety of digital assets from their previous works or content repositories and therefore it's necessary to import and export everything. Moreover, in the visual effects and animation industry pipeline content needs to be passed between different software back and forth \cite{usd, sdb}.

\item \textit{Debugging} and testing. While working on complex projects it is almost inevitable for some errors and bugs to appear. Some of these errors appear only for a certain order of actions and thus can be hard to reproduce. To isolate and fix these errors, it's important to track changes in the scene.

\item \textit{Distributed rendering} \cite{sdb} and explicit transfer of changes. Changes made in the DCCS should be visible as the same for every other computer participating in rendering. It's unacceptable to send the whole scene over the network, we need to \textit{track and transfer changes only}.

	
\end{enumerate}

\subsection{Domain-specific characteristics}

It's important to note that requirements listed above are specific to digital content creation software, since in other computer graphics software these requirements can only be found partially. Let's take a look at a few examples.

\begin{enumerate}

	\item Modern \underline{computer games}, \underline{simulators} and other similar applications are exceptionally complex software where the amount of visualized 3d content almost always does not fit into RAM or GPU memory. However, there is no parameters variability requirement in such applications: all digital content is optimized and prepared beforehand by both artists and programmers; mathematical models (materials, lights, geometry types, etc.) are fixed. Generally, you can't (and you don't need to) edit a 3d model or a texture in the process of playing a computer game. However, for DCCS it is the main functionality, as is the ability to extend materials' and lights' models with arbitrary parameters.

    \item In applications that usually work with \underline{databases} (like geographic information systems), most of the requirements described above are held, but with the exception of 1st requirement. Such applications are usually focused on a quick search for semantically important information and transactions that ensure the integrity of the database when multiple users are working simultaneously. There is generally no need to add big protions of information to the database. The distributed nature of the database (requirement #7) is also different since for a user there's usually no need to obtain the whole database (or a sizeable part of it). All the more so,  there's no need to replicate the database state on different computers (except for the sake of improving resiliency). It's completely opposite for a rendering system - to produce a photorealistic image renderer needs to have access to the whole scene and all computers participating in a distributed rendering process need to have the same state of the scene. On the other hand rendering system has no need for transaction mechanism since scene export takes place only from single application (which, however, \textbf{doesn't exclude possible multithreaded nature} of exporting process) 

    \item Interestingly, \underline{MS Word}, which we mentioned earlier, is, perhaps, the closest analogue of the DCCS. In such application only requirements #2 and #7 don't need to be hold - it's quite rare for a document to not fit into RAM or require a computational cluster. But in 3d editors it's a common situation.

\end{enumerate}

\section{Previous work}

\paragraph{Wavefront OBJ, Filmbox FBX and others}
The most basic way to transfer digital content between applications is to use binary or text files with strictly defined format. For example, simple but limited OBJ file format \cite{obj_format} or more complex and flexible FBX \cite{adsk_fbx}. In the case of strictly defined file formats there's always a trade-off between flexibility and complexity. The problem is that it's impossible to predict what features DCCS or rendering engine developers will  need in the future - what parameters would materials have, will there be new forms of geometry and light sources, etc.

\paragraph{OpenCollada and Alembic}

OpenCollada is a step forward compared to rigid file formats. The main difference is that OpenCollada defines only the standard for storing objects (called <<COLLADA>>) in XML \cite{opencollada} and allows developers to \textit{add new parameters}. It's possible since OpenCollada is an open source project and uses XML descriptions both for DCCS and rendering engine. More recent format with similar ideology is \textbf{Alembic}, presented on SIGGRAPH in 2011 \cite{sigg2011_abc}. The main focus of Alembic is to efficiently store complex animation - key framed or a product of any kind of simulation (fluid, cloth, etc.). However, support for materials in Alembic is still not available \cite{abc}, so one needs to use additional file formats like MaterialX \cite{mat_x} to transfer such data. 

\paragraph{Limitations of <<just files>>}

In the best case, only 2 or 3 of the requirements for modern digital content creation can be met with common file formats (most likely parameters variability, serialization and debugging/testing). To move forward there needs to be a way for working with large scenes, tracking and logging changes, and, most importantly, \textit{prompt transfer of changes} between different software (without rewriting the whole file).  

\paragraph{DRAM and DLL plugins}

The opposite of using files for the integration purpose is to transfer data structures through memory or shared memory via serialization as in \cite{serialization}. Dynamic Loading Library (DLL) plugins closely integrated with DCCS are worth mentioning specifically. These plugins usually directly call virtual functions of DCCS and thus don't import anything. For example, rendering plugin can call a function called $Shade$ for material evaluation. While possessing all of the advantages of being closely integrated, such approach limits the performance, scalability and does not guarantee correctness. You can never be sure that function $Shade$ is implemented sufficiently effective and that it does \textit{what render developers want} it to do. Moreover, this type of interaction between rendering system and DCCS strongly depends on DCCS in question \cite{barladian1} and that in turn complicates debugging process and integration into any other DCCS.

Both of these approaches, that we just described, are simple and usually provide sufficient performance. However, they can meet only two requirements - interactivity and parameters' variability.

\paragraph{SDB}

The comprehensive explanation of some of the requirements we listed in the beginning of our paper can be found in \cite{sdb}. Relying on their experience, authors of \cite{sdb} conclude that rendering engine should exist in the form of an API to the Scene Database (SDB) and describe functionality of such database. The implementation was not specified thus leaving an open space for future research. 

\paragraph{USD}

Universal Scene Description (USD) developed by Pixar was revealed to the public in 2016 \cite{usd}. This technology was primarily designed to address issues arising when different artists work on digital content for a big project like an animated feature film. USD assumes that digital content is created by \textit{different people in different applications}.

Basically, USD is represented by a set of files in a JSON-like format which are organized together by the means of references and compositions. For example, a composition of a location scene and a scene with animated character. This approach allows group of artists to work efficiently and simultaneously on a film shot components which are called layers and can be combined into a final result or reused in other shots. The resulting hierarchy of digital content created in various software allows tracking the history of scene creation and making several versions of the whole scene or it's components. Also, by using references and delayed loading USD makes it possible to work with large scenes \cite{usd}.

While USD satisfies a lot of requirements mentioned in the beginning of this article, it's difficult to use this technology directly as an integration layer with rendering system. The main problem is the absence of mechanism for fast updates. USD does not use global identifiers for objects and \textit{with every change} one needs to recursively check all of the file hierarchy. Moreover, the fact that any file can be overwritten complicates the implementation of distributed rendering, since some of the scene files on different rendering nodes can have different versions.

\paragraph{Multiverse}

Product called Multeverse being developed by J-Cube \cite{multiverse} was initially designed to tackle the problem of loading large scenes (alembic files in particular) and working with them in Autodesk Maya. To achieve this Multiverse takes over the job of loading and otherwise accessing geometry from the editor and implements delayed loading. The scene data is streamed directly into the rendering system or into OpenGL-based interactive viewport of the editor (i.e. Maya).

The downside of this approach is that Multiverse becomes too closely integrated with DCCS and \textit{reimplements} many of it's functions. If system like Multiverse is \textit{already integrated} with a certain DCCS then it could be used as integration layer between the DCCS in question and a rendering system. Otherwise, amount of work required to integrate Multiverse with a DCCS can be tenfold more than any other integration approach. Currently, Multiverse is available only for Maya and Katana, and does not support 3Ds Max, Blender or any of the CAD/CAM systems like Rhino or CATIA.

\paragraph{Bunsen}

Project Bunsen being developed by The Foundry \cite{bunsen_gtc} is a cloud-based software for assembling the final scene from different digital assets which supports import from various DCC and CAD/CAM software. 

Bunsen provides users with the ability to process imported data in the most suitable for the task at hand way through a \textit{data processing node graph}. The nodes in this graph can perform variety of operations such as converting splines into polygons, polygon mesh optimization, materials assignment, applying level-of-detail (LOD) techniques. If any of the assets used in a particular scene is changed, user needs to re-export it. Bunsen will recognize that one of the asset files has changed and will reload it and execute all nodes dependent on that asset again. The scene data is then prepared and streamed into selected imaging software -- interactive or <<offline>> renderer.

Since Bunsen is in the stage of active development, many details about it are not yet known to the public \cite{bunsen_gtc}. Nevertheless, the announcement of such system shows that there's a demand for novel integration solutions oriented on user-friendliness and emerging technologies as we pointed out in the begging of this paper.

\paragraph{Holodeck}

The Holodeck platform which is being developed by Nvidia \cite{holodeck_gtc, holodeck_gtcdc} and is currently in a beta-test stage can be thought of as an analogue Bunsen. Holodeck is a virual reality platform for collaborative design based on the Unreal Engine (just like Bunsen) and integrated with several Nvidia's proprietary technologies - PhysX, VRWorks, DesignWorks. The platform makes use of plugin mechanism to import digital content from 3ds Max and Maya (at the beta test stage) - models, materials and textures are first exported into specific file format and then imported to Holodeck. For content creation it is necessary to use Iray proprietary rendering system and materials in MDL (material definition language) from the vMaterials library \cite{holodeck_website}.

Users interaction mechanism utilizes client-server architecture available in Unreal Engine - one of the users acts as a host and others connect to a host as clients. Users' actions which modify the scene are synchronized with the server. The rendering process itself runs separately on users' computers, this fact accounts for quite high hardware requirements\cite{holodeck_website}. The simultaneous appearance of two software products (Bunsen and Holodeck) from large companies shows the demand for integration solutions that are oriented towards new features and user convenience, as we discussed at the beginning of this paper.

\subsection{Conclusion on existiong solutions}

In conclusion it should be said that all the methods and technologies discussed above are designed to some extent solve the problem of <<different formats>> or <<different representations>> of data, which in a sense can be called a fundamental problem.

The basis of this problem holds in the fact that most content export operations from DCCS to rendering system are time and resource consuming by definition. Therefore, it's difficult to somehow simplify or accelerate these operations. For example, loading a large texture from disk will be slow with any possible optimizations, and the creation of meshes from spline surfaces requires time and memory and is an often necessary in the process of export since most rendering systems can't work with spline surfaces directly. 

The obvious solution is to not re-export something that has already been exported at least once, and during long export processes try not to block the editor's calling program. To make this possible, all the changes to the rendered scene must be: \ textbf {(1) explicit}, \ textbf {(2) clearly marked} and \ textbf {(3) asynchronous} (so that the editor can respond to user actions during export).

\section{Suggested approach}

Our concept can be thought of as a hybrid of object oriented database and a version control system, (like Git or Subversion) with no-overwrite strategy of making changes. We believe that between the Editor and the rendering engine should exist special API. This API needs to fulfill the requirements listed in the beginning of the article in a simple and transparent manner (Fig. \ref{svn1}). 

\begin{figure}[h]
	\centering{\includegraphics[width=1.0\linewidth]{svn1.png}}
	\caption{Our intermediate layer (API) treats 3D content creation like working with source code -- make changes and explicitly commit them.}
	\label{svn1}
\end{figure}

A single state of the scene is a single XML file called <<state\_001.xml>> in some directory <<myscene>>. This file references several text or binary files in subdirectory <<data>> (usually geometry and textures). The internal XML structure is subdivided into <<library>> and <<scenes>>. The library contains references to all external files in subdirectory <<data>> and describes materials, lights and camera. A single scene is just a list of geometry instances. Instance is a reference to geometry object with custom transformation matrix and material remap list (if one should have instances with different materials).

Neither XML files, nor geometry or textures (thinking of them as external files in subdirectory <<data>>) should be actually saved to the hard drive. When the Editor passes them to the render, they are transferred through Operating System (OS) shared memory (Fig. \ref{svn2}). We will discuss this issue more specifically when we talk about <<Virtual Buffer>>. 

\begin{figure}[h]
	\centering{\includegraphics[width=1.0\linewidth]{svn2.png}}
	\caption{New or changed objects are always placed in the OS shared memory cache.}
	\label{svn2}
\end{figure}

\paragraph{Making changes}

A distinctive feature of the proposed technology is explicit tracking and recording the changes. Our API has 3 methods for each object (geometry, material, light and others) -- $Create$, $Open$ and $Close$ (like files in OS). $Create$ method makes an empty object. The pair of $Open$ / $Close$ allows to change the object. When user code (from the Editor side) calls $Open$ for some existing object, the copy of the object's XML parameters will be automatically created. Next, user can work with this copy, changing its XML parameters in RAM via pugixml \cite{pugixml}. When the work is finished $Close$ method should be called. From this point on, the new state of the object is considered as <<ready to commit>>, but it is still stored as a separate copy (Fig \ref{copyobj}, right).

\begin{figure}[h]
	\centering{\includegraphics[width=1.0\linewidth]{change.png}}
	\caption{ User of our API can only change an XML copy of an object (right). After <<commit>> this copy will replace original XML description (left) and form new state file. }
	\label{copyobj}
\end{figure}

We can say that if the current state of the object is stored in the file <<state\_001.xml>>, then the new state of the object is stored in a separate file <<change\_001.xml>> (Fig. \ref{svn1}). All these files do not need to be saved to the hard drive -- they can be stored in RAM in some dynamic structures (used by the pugixml library in our case). The user may change any number of any objects (including changing one object several times). The file <<change\_001.xml>> will contain only last changes for these objects. Thus it is important to note that file <<change\_001.xml>> will not contain any information about objects that were not actually changed.

Finally, the Editor calls $Commit$ to pass the new scene state to the renderer engine. The $Commit$ operation creates file <<state\_002.xml>> in which old objects from <<state\_001.xml>> are replaced by their copies from <<change\_001.xml>>. It should be clarified that during the execution of the $Commit$ operation, only new, modified objects and their XML nodes will be passed to the renderer to update their states inside render engine. Thus, the new render state will be consistent with file <<state\_002.xml>>, however, the state file itself is not analyzed. It should be mentioned that it does not matter in which order user changes objects. After $Commit$ operation has been called, the API will pass all changes to the render in a fixed, well-known order. Thus the rendering engine developers may rely on fixed and \textit{well-known} sequence of calls from our API. 

\paragraph{Ordered transfer of changes}

It should be noted that our integration layer (API) transmits to the renderer all the changes always in the same, strictly defined order, regardless of the order in which they were made by the user before the $Commit$ call. Due to this, the implementation of the renderer is greatly simplified, since it can rely on a known and predetermined sequence of calls from the top layer (integration layer), regardless of the order in which API calls are invoked in the application (aA simplified illustration of this process is shown in fig. \ref{calls}).

\begin{figure}[h]
	\centering{\includegraphics[width=1.0\linewidth]{calls.png}}
	\caption{An example of reordering calls by an intermediate layer. Take note of 2 points: (1) object #1 is transmitted to renderer earlier than object #2, despite the fact that object #2 was changed by user first; (2) before the function $GetImage$ was called, the object # 3 was not sent to the renderer because after updating the object #3, the $Commit$ operation was not called. Consequently, the resulting image will not contain changes to the third object, but this is what allows us to not block the editor. }
	\label{calls}
\end{figure}

Thus, to <<connect>> a specific rendering implementation to the integration layer, the renderer needs to define its implementation of the interface specified by the API. The methods of the interface, called <<Render Driver>>, as already mentioned, have a well-defined sequence of calls. The list of methods includes: loading and updating the components of the scene library (meshes, materials, textures, light sources, camera), updating the render settings, instantiating meshes and light sources, rendering the scene, transfering the image to the upper level (the user application, DCCS), and <<preparatory>> methods called directly before and after the instantiating objects and library updates. The renderer can Define only those methods that correspond to the capabilities it supports.

\paragraph{Testing}

This approach makes debugging and testing easier. The developers of the rendering system do not need to know about the top level application. For them, only the sequence of calls transmitted from the intermediate layer to the render is important. A similar approach is widely used when debugging OpenGL, DirectX and Vulkan drivers \cite{apitrace}. However, it should be noted that when debugging a program using the mentioned API, the sequence calls is simply written, and then \cite{apitrace} played back - so, there is no ordering. This is different in our API - because of ordering, we are able to create a separate set of tests for the integration layer and render engines, testing the <<input>> (the functionality of the API itself) and the <<output>> (the functionality of the render engine) separately, which is impossible with traditional graphics APIs.

We would like to note that the described approach to testing differs from traditional approaches for testing computer graphics systems due to the presence of an intermediate layer. For example, in \cite {testing} for testing purposes, user actions are recorded and played back in a specific application software. In the opposite of this, our API records only the <<input>> coming from the application software into our API and allows testing the render engine without binding it to a specific software.


\paragraph{Multithreaded export}

It should be noted that the proposed concept easily allows us to make the process of exporting content from DCCS multithreaded:

\begin{enumerate}
  
  \item when creating an object using the $Create$ method, you need to use the atomic increment operation $atomic\_add(pCounter, 1)$ to get the ID of the new object;
  
  \item create a unique mutex or spin-lock for each object;
  
  \item when calling the $Open$ method, the mutex blocks access to the same object from other threads;
  
  \item when the $Close$ method is called, the mutex lock is released;
  
  \item when adding an object to the virtual buffer to calculate the address in the buffer, you must use the atomic addition operation $atomic\_add(pAddr, sizeof (obj))$;
  
  \item when allocating memory for dynamic data structures (for example, those using pugixml), it is necessary to use a mechanism similar to the previous point based on atomic operations.
  
\end{enumerate}

\paragraph{Virtual Buffer}\label{virtualbuffer}

To handle big data (geometry and textures) we use a concept of virtual append-buffer with infinite size. The buffer can append linear data blocks to its end. But only last $N$ Megabytes are put into RAM. The rest of the buffer is flushed to a hard drive as a set of chunks -- binary files in $data$ subdirectory (Fig \ref{svn2}). It should be outlined that any new or changed object is \textit{always} appended to the end of the buffer. This is due to the no-overwrite strategy. If you need to change a texture placed in <<chunk\_036.bin>>, you'll have to create a copy of this texture and place it into <<chunk\_037.bin>>. The XML description will change the reference from 36 chunk to 37, but both chunks will exist in the buffer <<until the end of time>>. This way we can be sure that most of the objects that user works with are placed in RAM. For common execution scenario any object that was flushed to a hard drive (in the green rectangle in Fig. \ref{svn2}) may only be in 2 states inside renderer. First state: this object has already been passed to the renderer and has valid state inside of it. Second: the object will not be passed to the renderer at all since it is not needed anymore (details further). 

\paragraph{Why no-overwrite}

We use no-overwrite strategy due to support for network rendering. Allowing to overwrite any file would create a possibility that on one machine this file will have an old state, and on the other a new one. No-overwrite strategy guarantees this can never happen. If the texture file is not on the local machine, it means that the file has not been transferred to this machine yet (and then the renderer waits for the transfer of this file), or it would not be transferred, because the system has a new state for the same texture. In the latter case, the renderer must go to the next state of the scene and wait for the new file of the same texture to be transferred and ignore the old one.

\section{Results and discussion}

The API is integrated with a freeware rendering system Hydra Renderer and two DCC applications -- 3ds Max and Fabric Engine. The suggested concept of DCC application and renderer integration meets all of the requirements that we listed in the beginning of the paper. Among the drawbacks of the proposed approach, one can note an obvious overrun of disk memory caused by the need to store complete copies of different versions of the same object. Nevertheless, effective network transfer of such copies is possible (for example, by using paged memory for virtual buffer and COW \cite{cow} for each page) but wasn't considered by us.

It should be noted that in the proposed approach the rollback to some previous system state in most cases is equivalent to loading this state <<from scratch>>, -- a complete analysis of some file <<state\_N.xml>> and loading most of the \textit{needed} (i.e. we don't have to scan the whole virtual buffer and can load only necessary chunks) data for geometry and textures from the hard drive.

\paragraph{Acknowledgments}

This work is sponsored by RFBR 16-31-60048 <<mol\_a\_dk>>.


\begin{thebibliography}{99}

\small

\bibitem{hpg}
{\em High Performance Graphics.} //  The international forum for performance-oriented graphics systems research. 2017. 

\bibitem{outofcore} Ingo Wald Andreas Dietrich and Phlipp Slusallek.
An Interactive Out-of-Core Rendering Framework for Visualizing Massively Complex Models. // Eurographics Symposium on Rendering (2004).

\bibitem{mrhair} Marsel Khadiyev. Ornatrix mentalRay shaders. // third party plugin for mental ray for hairs. {URL = https://ephere.com/plugins/autodesk/max/ornatrix/docs/1286.html} -- 

\bibitem{vr_app2}
{Danny Rollings}
{\em The Future of 3D Modelling} // https://garagefarm.net/en/the-future-of-3d-modelling/   

\bibitem{obj_format} Wavefront obj file format.

\bibitem{adsk_fbx}
{\em Autodesk FBX format} 
{URL = https://www.autodesk.com/products/fbx/overview}  

\bibitem{opencollada}
{\em OpenCollada} A project to join efforts using COLLADA within various DCC tools. 
{URL = http://www.opencollada.org}  

\bibitem{barladian1}
{Барладян Б.Х., Волобой А.Г., Шапиро Л.З.}
{\em Построение реалистичных изображений в системах автоматизированного проектирования} // Труды 23-й Международной Конференции по Компьютерной Графике и Зрению, Институт автоматики и процессов управления ДВО РАН, 16-20 сентября 2013 года, с.148-151. 

\bibitem{sdb}
Дерябин Н.Б., Денисов Е.Ю. {\em Объектно-ориентированная инфраструктура систем компьютерной графики } // Труды 17-ой международной конференции по компьютерной графике и зрению, Россия, Московский Государственный Университет, июнь 23-27, 2007, с. 289-292.


\bibitem{bunsen_gtc}
{Adam Glick, George Matos}
{\em Scalable Enterprise Visualization, GTC (GPU Technology Conference) 2017, March 26-29, Silicon Valley, presentation ID S7474} 	

\bibitem{holodeck_gtc}
{Jensen Huang}
{\em Keynote, GTC (GPU Technology Conference) 2017, March 26-29, Silicon Valley, presentation ID S7820} 

\bibitem{holodeck_gtcdc}
{David Weinstein}
{\em Nvidia Holodeck, GTC (GPU Technology Conference) DC 2017, November 1-2, Washington, D.C.} 


\bibitem{holodeck_website}
{\em Nvidia Holodeck} 
{URL = https://www.nvidia.com/en-us/design-visualization/technologies/holodeck}  


\bibitem{sigg2011_abc}
{\em Siggraph 2011, Alembic talk}
{URL = http://www.siggraph.org/s2011/content/practical-integration-alembic} 

\bibitem{abc}
{\em Alembic}
{URL = http://www.alembic.io/}  

\bibitem{usd}
{\em Pixar USD}
{URL = https://graphics.pixar.com/usd/docs/index.html}  

\bibitem{multiverse}
{\em Multiverse}
{URL = http://multi-verse.io/}           
     
\bibitem{pugixml} Arseny Kapoulkine. {\em A light-weight C++ XML processing library.} {URL = http://pugixml.org}        

\bibitem{apitrace} {\em apitrace.} Tools for tracing OpenGL, Direct3D, and other graphics APIs. URL = http://apitrace.github.io/

\bibitem{testing} А.Г. Волобой, Е.Ю. Денисов, Б.Х. Барладян. 
{\em Тестирование систем моделирования освещенности и синтеза реалистичных изображений // Программирование, № 4, 2014, с.13-22.} English translation: A. G. Voloboi, E. Yu. Denisov, B. Kh. Barladyan. {\em Testing of Systems for Illumination Simulation and Synthesis of Realistic Images} // Programming and Computer Software, 2014, Vol. 40, № 4, pp. 166–173. DOI: 10.1134/S0361768814040094


\bibitem{cow} Bovet, Daniel Pierre; Cesati, Marco. Understanding the Linux Kernel. // O'Reilly Media, Inc. p. 295. ISBN 9780596002138. 2002.    


\end{thebibliography}

\label{lastpage}
\end{document}






